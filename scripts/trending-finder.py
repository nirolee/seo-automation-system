"""
çƒ­è¯è‡ªåŠ¨å‘ç°å·¥å…·
åŠŸèƒ½ï¼šè‡ªåŠ¨ä»å¤šä¸ªæ¥æºå‘ç°å½“å‰çƒ­é—¨å…³é”®è¯å’Œè¶‹åŠ¿è¯é¢˜
æ•°æ®æºï¼šGoogle Trends, ç™¾åº¦çƒ­æœ, Reddit, çŸ¥ä¹çƒ­æ¦œç­‰
"""

# -*- coding: utf-8 -*-
import sys
import io

# ä¿®å¤Windowsä¸­æ–‡ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import csv
from collections import Counter

class TrendingKeywordFinder:
    """çƒ­è¯å‘ç°å™¨"""

    def __init__(self, use_proxy=True, proxy_port=7890):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.all_trends = []

        # è®¾ç½®ä»£ç†ï¼ˆç”¨äºè®¿é—®Googleï¼‰
        if use_proxy:
            self.proxies = {
                'http': f'http://127.0.0.1:{proxy_port}',
                'https': f'http://127.0.0.1:{proxy_port}'
            }
            print(f"âœ… å·²å¯ç”¨ä»£ç†: 127.0.0.1:{proxy_port}")
        else:
            self.proxies = None
            print("âš ï¸  æœªä½¿ç”¨ä»£ç†")

    def get_google_trends_daily(self, geo='US'):
        """
        è·å–Google Trendsæ¯æ—¥çƒ­æœ
        geo: å›½å®¶ä»£ç  (US=ç¾å›½, CN=ä¸­å›½, GB=è‹±å›½ç­‰)
        """
        print(f"\n[1/6] æ­£åœ¨è·å–Googleæ¯æ—¥çƒ­æœ ({geo})...")

        try:
            # Google Trends RSS Feedï¼ˆå…è´¹ï¼ï¼‰
            url = f"https://trends.google.com/trends/trendingsearches/daily/rss?geo={geo}"
            response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=15)

            soup = BeautifulSoup(response.content, 'xml')
            items = soup.find_all('item')

            trends = []
            for item in items[:20]:  # å–å‰20ä¸ª
                title = item.find('title')
                traffic = item.find('ht:approx_traffic')

                if title:
                    trend = {
                        'keyword': title.text.strip(),
                        'source': f'Google Trends ({geo})',
                        'traffic': traffic.text if traffic else 'N/A',
                        'category': 'çƒ­æœ',
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M')
                    }
                    trends.append(trend)

            print(f"   âœ… æ‰¾åˆ° {len(trends)} ä¸ªGoogleçƒ­æœè¯")
            return trends

        except Exception as e:
            print(f"   âš ï¸  è·å–å¤±è´¥: {e}")
            return []

    def get_baidu_hot(self):
        """è·å–ç™¾åº¦çƒ­æœæ¦œ"""
        print(f"\n[2/6] æ­£åœ¨è·å–ç™¾åº¦çƒ­æœæ¦œ...")

        try:
            url = "https://top.baidu.com/board?tab=realtime"
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            trends = []
            # ç™¾åº¦çƒ­æœçš„HTMLç»“æ„å¯èƒ½å˜åŒ–ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€ç‰ˆæœ¬
            items = soup.find_all('div', class_='c-single-text-ellipsis')

            for item in items[:20]:
                keyword = item.text.strip()
                if keyword and len(keyword) > 2:
                    trends.append({
                        'keyword': keyword,
                        'source': 'ç™¾åº¦çƒ­æœ',
                        'traffic': 'N/A',
                        'category': 'çƒ­æœ',
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M')
                    })

            print(f"   âœ… æ‰¾åˆ° {len(trends)} ä¸ªç™¾åº¦çƒ­æœè¯")
            return trends

        except Exception as e:
            print(f"   âš ï¸  è·å–å¤±è´¥: {e}")
            return []

    def get_reddit_trending(self, subreddit='all'):
        """è·å–Redditçƒ­é—¨è¯é¢˜"""
        print(f"\n[3/6] æ­£åœ¨è·å–Redditçƒ­é—¨è¯é¢˜...")

        try:
            url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=25"
            response = requests.get(url, headers={**self.headers, 'User-Agent': 'TrendFinder/1.0'}, proxies=self.proxies, timeout=15)
            data = response.json()

            trends = []
            for post in data['data']['children'][:20]:
                post_data = post['data']
                title = post_data['title']
                score = post_data['score']

                trends.append({
                    'keyword': title,
                    'source': f'Reddit r/{subreddit}',
                    'traffic': f'{score} upvotes',
                    'category': post_data.get('subreddit', 'general'),
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M')
                })

            print(f"   âœ… æ‰¾åˆ° {len(trends)} ä¸ªRedditçƒ­é—¨è¯é¢˜")
            return trends

        except Exception as e:
            print(f"   âš ï¸  è·å–å¤±è´¥: {e}")
            return []

    def get_zhihu_hot(self):
        """è·å–çŸ¥ä¹çƒ­æ¦œ"""
        print(f"\n[4/6] æ­£åœ¨è·å–çŸ¥ä¹çƒ­æ¦œ...")

        try:
            # çŸ¥ä¹çƒ­æ¦œAPIï¼ˆå¯èƒ½éœ€è¦æ›´æ–°ï¼‰
            url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total"
            response = requests.get(url, headers=self.headers, timeout=10)
            data = response.json()

            trends = []
            for item in data.get('data', [])[:20]:
                target = item.get('target', {})
                title = target.get('title', '')

                if title:
                    trends.append({
                        'keyword': title,
                        'source': 'çŸ¥ä¹çƒ­æ¦œ',
                        'traffic': f"{item.get('detail_text', 'N/A')}",
                        'category': 'çƒ­æœ',
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M')
                    })

            print(f"   âœ… æ‰¾åˆ° {len(trends)} ä¸ªçŸ¥ä¹çƒ­æ¦œè¯")
            return trends

        except Exception as e:
            print(f"   âš ï¸  è·å–å¤±è´¥: {e}")
            return []

    def get_google_trends_rising(self, geo='US', category=''):
        """
        è·å–Google Trendsä¸Šå‡è¶‹åŠ¿è¯
        category: åˆ†ç±» (e.g., 'business', 'technology', 'health')
        """
        print(f"\n[5/6] æ­£åœ¨è·å–Googleä¸Šå‡è¶‹åŠ¿è¯...")

        try:
            # ä½¿ç”¨pytrendsåº“ä¼šæ›´å¥½ï¼Œä½†è¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬
            # å®é™…ä½¿ç”¨æ—¶å»ºè®®å®‰è£…: pip install pytrends

            # è¿™é‡Œæä¾›ä¸€ä¸ªåŸºäºRSSçš„æ›¿ä»£æ–¹æ¡ˆ
            trends = []
            print(f"   ğŸ’¡ æç¤º: å®‰è£…pytrendsåº“å¯è·å–æ›´å¤šæ•°æ®")
            print(f"      å‘½ä»¤: pip install pytrends")

            return trends

        except Exception as e:
            print(f"   âš ï¸  è·å–å¤±è´¥: {e}")
            return []

    def get_youtube_trending(self, region='US'):
        """è·å–YouTubeçƒ­é—¨è§†é¢‘æ ‡é¢˜ï¼ˆå¯æå–å…³é”®è¯ï¼‰"""
        print(f"\n[6/6] æ­£åœ¨è·å–YouTubeçƒ­é—¨è¯é¢˜...")

        try:
            # YouTube RSS Feed
            url = "https://www.youtube.com/feed/trending"
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            trends = []
            # æå–è§†é¢‘æ ‡é¢˜ä½œä¸ºçƒ­é—¨è¯é¢˜
            scripts = soup.find_all('script')

            # YouTubeçš„æ•°æ®åœ¨JavaScriptä¸­ï¼Œéœ€è¦è§£æ
            # è¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬
            print(f"   ğŸ’¡ æç¤º: YouTubeéœ€è¦APIå¯†é’¥æ‰èƒ½è·å–æ›´å‡†ç¡®æ•°æ®")

            return trends

        except Exception as e:
            print(f"   âš ï¸  è·å–å¤±è´¥: {e}")
            return []

    def extract_keywords_from_trends(self, trends):
        """ä»çƒ­é—¨è¯é¢˜ä¸­æå–å…³é”®è¯"""
        print(f"\nğŸ“Š ä» {len(trends)} ä¸ªè¯é¢˜ä¸­æå–å…³é”®è¯...")

        all_words = []
        for trend in trends:
            # ç§»é™¤æ ‡ç‚¹å’Œåˆ†è¯
            text = trend['keyword'].lower()
            # ç®€å•åˆ†è¯ï¼ˆè‹±æ–‡æŒ‰ç©ºæ ¼ï¼Œä¸­æ–‡éœ€è¦jiebaï¼‰
            words = text.split()
            all_words.extend([w for w in words if len(w) > 3])

        # ç»Ÿè®¡é«˜é¢‘è¯
        word_freq = Counter(all_words)
        top_keywords = word_freq.most_common(30)

        print(f"   âœ… æå–å‡º {len(top_keywords)} ä¸ªé«˜é¢‘å…³é”®è¯")
        return top_keywords

    def categorize_trends(self, trends):
        """å°†è¶‹åŠ¿è¯åˆ†ç±»åˆ°ä¸åŒåˆ©åŸºå¸‚åœº"""
        print(f"\nğŸ·ï¸  å¯¹çƒ­è¯è¿›è¡Œåˆ†ç±»...")

        categories = {
            'ç§‘æŠ€æ•°ç ': ['tech', 'phone', 'laptop', 'software', 'ai', 'app', 'game', 'iphone', 'android'],
            'å¥åº·å¥èº«': ['health', 'fitness', 'diet', 'workout', 'weight', 'yoga', 'nutrition'],
            'é‡‘èç†è´¢': ['stock', 'crypto', 'bitcoin', 'investment', 'money', 'finance', 'trading'],
            'ç”Ÿæ´»å®¶å±…': ['home', 'kitchen', 'furniture', 'decor', 'garden', 'cleaning'],
            'æ—¶å°šç¾å¦†': ['fashion', 'beauty', 'makeup', 'skincare', 'clothing', 'style'],
            'æ—…æ¸¸': ['travel', 'hotel', 'flight', 'vacation', 'trip', 'destination'],
            'ç¾é£Ÿ': ['food', 'recipe', 'cooking', 'restaurant', 'coffee', 'wine'],
            'æ•™è‚²': ['course', 'learn', 'tutorial', 'education', 'study', 'training'],
            'å¨±ä¹': ['movie', 'music', 'celebrity', 'tv', 'show', 'entertainment']
        }

        categorized = {cat: [] for cat in categories.keys()}
        categorized['å…¶ä»–'] = []

        for trend in trends:
            keyword_lower = trend['keyword'].lower()
            matched = False

            for category, keywords in categories.items():
                if any(kw in keyword_lower for kw in keywords):
                    categorized[category].append(trend)
                    matched = True
                    break

            if not matched:
                categorized['å…¶ä»–'].append(trend)

        # æ‰“å°åˆ†ç±»ç»“æœ
        for category, items in categorized.items():
            if items:
                print(f"   {category}: {len(items)}ä¸ª")

        return categorized

    def score_trend_opportunity(self, trend):
        """è¯„ä¼°çƒ­è¯çš„å•†ä¸šæœºä¼šåˆ†æ•°"""
        score = 0
        keyword = trend['keyword'].lower()

        # 1. å•†ä¸šæ„å›¾è¯
        commercial_keywords = ['best', 'buy', 'review', 'vs', 'how to', 'top', 'cheap', 'price']
        for ck in commercial_keywords:
            if ck in keyword:
                score += 20
                break

        # 2. é•¿åº¦é€‚ä¸­
        word_count = len(keyword.split())
        if 2 <= word_count <= 5:
            score += 15

        # 3. åŒ…å«æ•°å­—
        if any(char.isdigit() for char in keyword):
            score += 10

        # 4. æµé‡æŒ‡æ ‡
        traffic = trend.get('traffic', '')
        if traffic != 'N/A' and traffic:
            # æå–æ•°å­—
            import re
            numbers = re.findall(r'\d+', str(traffic))
            if numbers:
                traffic_num = int(numbers[0])
                if traffic_num > 100000:
                    score += 30
                elif traffic_num > 50000:
                    score += 20
                elif traffic_num > 10000:
                    score += 10

        # 5. æ—¶æ•ˆæ€§ï¼ˆæ–°é—»ç±»çƒ­è¯åˆ†æ•°ä½ï¼‰
        news_keywords = ['æ­»', 'å»ä¸–', 'äº‹æ•…', 'æ–°é—»', 'å¿«è®¯']
        if any(nk in keyword for nk in news_keywords):
            score -= 20

        return max(0, min(100, score))

    def generate_niche_ideas(self, categorized_trends):
        """æ ¹æ®çƒ­è¯ç”Ÿæˆåˆ©åŸºå¸‚åœºå»ºè®®"""
        print(f"\nğŸ’¡ ç”Ÿæˆåˆ©åŸºå¸‚åœºå»ºè®®...")

        suggestions = []

        for category, trends in categorized_trends.items():
            if not trends or category == 'å…¶ä»–':
                continue

            # æ‰¾å‡ºè¯¥åˆ†ç±»ä¸‹æœ€é«˜åˆ†çš„è¶‹åŠ¿
            scored_trends = []
            for trend in trends:
                score = self.score_trend_opportunity(trend)
                scored_trends.append((trend, score))

            scored_trends.sort(key=lambda x: x[1], reverse=True)

            if scored_trends and scored_trends[0][1] > 30:  # åªæ¨èé«˜åˆ†çš„
                top_trend, top_score = scored_trends[0]

                suggestion = {
                    'category': category,
                    'seed_keyword': top_trend['keyword'],
                    'opportunity_score': top_score,
                    'related_trends': [t[0]['keyword'] for t in scored_trends[1:4]],
                    'suggested_domain': self._generate_domain_idea(top_trend['keyword']),
                    'content_ideas': self._generate_content_ideas(top_trend['keyword'])
                }
                suggestions.append(suggestion)

        return suggestions

    def _generate_domain_idea(self, keyword):
        """æ ¹æ®å…³é”®è¯ç”ŸæˆåŸŸåå»ºè®®"""
        # æå–æ ¸å¿ƒè¯
        words = keyword.lower().split()[:2]
        core = ''.join([w for w in words if len(w) > 3])

        return [
            f"{core}hub.com",
            f"{core}guide.com",
            f"best{core}.com",
            f"{core}review.com"
        ]

    def _generate_content_ideas(self, keyword):
        """ç”Ÿæˆå†…å®¹åˆ›æ„"""
        return [
            f"Best {keyword} in 2024",
            f"How to choose {keyword}",
            f"{keyword} review and comparison",
            f"Top 10 {keyword} for beginners",
            f"{keyword} buying guide"
        ]

    def export_results(self, trends, categorized, suggestions, filename='trending_keywords'):
        """å¯¼å‡ºç»“æœåˆ°å¤šä¸ªæ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # 1. å¯¼å‡ºæ‰€æœ‰çƒ­è¯
        with open(f'{filename}_{timestamp}.csv', 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['keyword', 'source', 'traffic', 'category', 'opportunity_score', 'timestamp']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()

            for trend in trends:
                trend['opportunity_score'] = self.score_trend_opportunity(trend)
                writer.writerow(trend)

        print(f"\nğŸ’¾ å·²å¯¼å‡ºåˆ°: {filename}_{timestamp}.csv")

        # 2. å¯¼å‡ºåˆ©åŸºå¸‚åœºå»ºè®®
        with open(f'{filename}_suggestions_{timestamp}.txt', 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("åˆ©åŸºå¸‚åœºå»ºè®®æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")

            for i, sug in enumerate(suggestions, 1):
                f.write(f"\nå»ºè®® #{i}: {sug['category']}\n")
                f.write(f"{'='*40}\n")
                f.write(f"æ ¸å¿ƒå…³é”®è¯: {sug['seed_keyword']}\n")
                f.write(f"æœºä¼šè¯„åˆ†: {sug['opportunity_score']}/100\n")
                f.write(f"\nç›¸å…³çƒ­è¯:\n")
                for rt in sug['related_trends']:
                    f.write(f"  - {rt}\n")
                f.write(f"\næ¨èåŸŸå:\n")
                for domain in sug['suggested_domain'][:2]:
                    f.write(f"  - {domain}\n")
                f.write(f"\nå†…å®¹åˆ›æ„:\n")
                for idea in sug['content_ideas']:
                    f.write(f"  - {idea}\n")
                f.write("\n")

        print(f"ğŸ’¾ å·²å¯¼å‡ºåˆ°: {filename}_suggestions_{timestamp}.txt")

    def run(self, regions=['US', 'CN']):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        print("=" * 60)
        print("ğŸ”¥ çƒ­è¯è‡ªåŠ¨å‘ç°å·¥å…·")
        print("=" * 60)

        all_trends = []

        # æ”¶é›†å„ä¸ªæ¥æºçš„çƒ­è¯
        for region in regions:
            if region == 'US':
                trends = self.get_google_trends_daily('US')
                all_trends.extend(trends)

                reddit_trends = self.get_reddit_trending('all')
                all_trends.extend(reddit_trends)

            elif region == 'CN':
                baidu_trends = self.get_baidu_hot()
                all_trends.extend(baidu_trends)

                zhihu_trends = self.get_zhihu_hot()
                all_trends.extend(zhihu_trends)

        # åˆ†ç±»
        categorized = self.categorize_trends(all_trends)

        # ç”Ÿæˆå»ºè®®
        suggestions = self.generate_niche_ideas(categorized)

        # æ˜¾ç¤ºå»ºè®®
        print("\n" + "=" * 60)
        print("ğŸ¯ åˆ©åŸºå¸‚åœºæœºä¼šæ¨è")
        print("=" * 60)

        suggestions.sort(key=lambda x: x['opportunity_score'], reverse=True)

        for i, sug in enumerate(suggestions[:5], 1):
            print(f"\nã€æ¨è #{i}ã€‘{sug['category']} - è¯„åˆ†: {sug['opportunity_score']}/100")
            print(f"   æ ¸å¿ƒè¯: {sug['seed_keyword']}")
            print(f"   åŸŸåå»ºè®®: {sug['suggested_domain'][0]}")
            print(f"   ç›¸å…³çƒ­è¯: {', '.join(sug['related_trends'][:3])}")

        # å¯¼å‡º
        self.export_results(all_trends, categorized, suggestions)

        print("\n" + "=" * 60)
        print("âœ… å®Œæˆï¼")
        print("=" * 60)

        return {
            'trends': all_trends,
            'categorized': categorized,
            'suggestions': suggestions
        }


if __name__ == '__main__':
    print("\næ˜¯å¦ä½¿ç”¨ä»£ç†è®¿é—®Google? (æ¨è: æ˜¯)")
    use_proxy_input = input("ä½¿ç”¨ä»£ç† (y/n) [é»˜è®¤: y]: ").strip().lower() or 'y'
    use_proxy = use_proxy_input == 'y'

    proxy_port = 7890
    if use_proxy:
        proxy_input = input(f"ä»£ç†ç«¯å£ [é»˜è®¤: {proxy_port}]: ").strip()
        if proxy_input:
            proxy_port = int(proxy_input)

    finder = TrendingKeywordFinder(use_proxy=use_proxy, proxy_port=proxy_port)

    print("\nè¯·é€‰æ‹©å¸‚åœº:")
    print("1. ç¾å›½å¸‚åœº (US)")
    print("2. ä¸­å›½å¸‚åœº (CN)")
    print("3. åŒå¸‚åœº (US + CN)")

    choice = input("\nè¯·è¾“å…¥é€‰æ‹© [é»˜è®¤: 3]: ").strip() or '3'

    regions = []
    if choice == '1':
        regions = ['US']
    elif choice == '2':
        regions = ['CN']
    else:
        regions = ['US', 'CN']

    results = finder.run(regions=regions)

    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. æŸ¥çœ‹ç”Ÿæˆçš„CSVæ–‡ä»¶ï¼Œæ‰¾åˆ°æ„Ÿå…´è¶£çš„çƒ­è¯")
    print("   2. ä½¿ç”¨ keyword-digger.py æ·±å…¥æŒ–æ˜è¯¥çƒ­è¯")
    print("   3. åˆ†æç«äº‰å¯¹æ‰‹ï¼Œåˆ¶å®šå»ºç«™è®¡åˆ’")
