"""
å…è´¹å…³é”®è¯æŒ–æ˜å·¥å…· - å®Œæ•´ç‰ˆ
åŠŸèƒ½ï¼š
1. ä»Google/ç™¾åº¦è‡ªåŠ¨å»ºè®®æŒ–æ˜å…³é”®è¯
2. åˆ†æç«äº‰å¯¹æ‰‹ç½‘ç«™
3. è¯„ä¼°å…³é”®è¯ä»·å€¼
4. ç”Ÿæˆç«™ç‚¹å»ºè®®
"""

# -*- coding: utf-8 -*-
import sys
import io

# ä¿®å¤Windowsä¸­æ–‡ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import requests
import json
from urllib.parse import quote, urlparse
import time
from bs4 import BeautifulSoup
import re
from collections import Counter
import csv
from datetime import datetime

class KeywordDigger:
    """å…è´¹å…³é”®è¯æŒ–æ˜å™¨"""

    def __init__(self, use_proxy=True, proxy_port=7890):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        # è®¾ç½®ä»£ç†ï¼ˆç”¨äºè®¿é—®Googleï¼‰
        if use_proxy:
            self.proxies = {
                'http': f'http://127.0.0.1:{proxy_port}',
                'https': f'http://127.0.0.1:{proxy_port}'
            }
            print(f"âœ… å·²å¯ç”¨ä»£ç†: 127.0.0.1:{proxy_port}")
        else:
            self.proxies = None
            print("âš ï¸  æœªä½¿ç”¨ä»£ç†")

    def get_google_suggestions(self, seed_keyword, language='en'):
        """è·å–Googleæœç´¢å»ºè®®"""
        print(f"ğŸ” æ­£åœ¨ä»Googleè·å–å»ºè®®è¯...")
        suggestions = set()

        url = "http://suggestqueries.google.com/complete/search"

        # ç­–ç•¥1: åœ¨å…³é”®è¯ååŠ a-z
        for char in 'abcdefghijklmnopqrstuvwxyz':
            params = {
                'client': 'firefox',
                'q': f'{seed_keyword} {char}',
                'hl': language
            }
            try:
                response = requests.get(url, params=params, proxies=self.proxies, timeout=5)
                data = json.loads(response.text)
                if len(data) > 1:
                    suggestions.update(data[1])
                time.sleep(0.3)
            except Exception as e:
                print(f"   âš ï¸  è¯·æ±‚å¤±è´¥: {char}")
                continue

        # ç­–ç•¥2: é—®é¢˜è¯å‰ç¼€
        question_words = ['how to', 'what is', 'why', 'when', 'where', 'best', 'top']
        for qw in question_words:
            params = {
                'client': 'firefox',
                'q': f'{qw} {seed_keyword}',
                'hl': language
            }
            try:
                response = requests.get(url, params=params, proxies=self.proxies, timeout=5)
                data = json.loads(response.text)
                if len(data) > 1:
                    suggestions.update(data[1])
                time.sleep(0.3)
            except:
                continue

        print(f"   âœ… æ‰¾åˆ° {len(suggestions)} ä¸ªGoogleå»ºè®®è¯")
        return list(suggestions)

    def get_baidu_suggestions(self, seed_keyword):
        """è·å–ç™¾åº¦æœç´¢å»ºè®®ï¼ˆä¸­æ–‡ï¼‰"""
        print(f"ğŸ” æ­£åœ¨ä»ç™¾åº¦è·å–å»ºè®®è¯...")
        suggestions = set()

        url = "https://www.baidu.com/sugrec"

        for char in 'abcdefghijklmnopqrstuvwxyz0123456789':
            params = {
                'prod': 'pc',
                'wd': f'{seed_keyword} {char}',
                'cb': 'jQuery'
            }
            try:
                response = requests.get(url, params=params, proxies=self.proxies, timeout=5)
                # è§£æè¿”å›çš„JSONP
                text = response.text
                if 'jQuery' in text:
                    json_str = text[text.find('(')+1:text.rfind(')')]
                    data = json.loads(json_str)
                    if 'g' in data:
                        for item in data['g']:
                            suggestions.add(item['q'])
                time.sleep(0.3)
            except:
                continue

        print(f"   âœ… æ‰¾åˆ° {len(suggestions)} ä¸ªç™¾åº¦å»ºè®®è¯")
        return list(suggestions)

    def search_google_for_competitors(self, keyword, num_results=10):
        """æœç´¢Googleæ‰¾åˆ°æ’åé å‰çš„ç«äº‰å¯¹æ‰‹"""
        print(f"ğŸ” æœç´¢Googleæ‰¾ç«äº‰å¯¹æ‰‹: {keyword}")

        # æ³¨æ„ï¼šç›´æ¥çˆ¬Googleå¯èƒ½è¢«å°ï¼Œå»ºè®®ä½¿ç”¨ä»£ç†æˆ–è€…æ‰‹åŠ¨è¾“å…¥
        # è¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬

        url = f"https://www.google.com/search?q={quote(keyword)}&num={num_results}"

        try:
            response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            # æå–æœç´¢ç»“æœURL
            competitors = []
            for g in soup.find_all('div', class_='g'):
                link = g.find('a')
                if link and 'href' in link.attrs:
                    href = link['href']
                    if href.startswith('http'):
                        domain = urlparse(href).netloc
                        competitors.append({
                            'url': href,
                            'domain': domain,
                            'title': g.find('h3').text if g.find('h3') else ''
                        })

            print(f"   âœ… æ‰¾åˆ° {len(competitors)} ä¸ªç«äº‰ç½‘ç«™")
            return competitors

        except Exception as e:
            print(f"   âš ï¸  Googleæœç´¢å¤±è´¥: {e}")
            print(f"   ğŸ’¡ å»ºè®®ï¼šæ‰‹åŠ¨æœç´¢ '{keyword}' å¹¶æä¾›ç«äº‰å¯¹æ‰‹URL")
            return []

    def analyze_competitor_site(self, url):
        """æ·±åº¦åˆ†æç«äº‰å¯¹æ‰‹ç½‘ç«™"""
        print(f"\nğŸ“Š åˆ†æç½‘ç«™: {url}")

        analysis = {
            'url': url,
            'domain': urlparse(url).netloc,
            'title': '',
            'keywords': [],
            'content_structure': {},
            'monetization': [],
            'tech_stack': [],
            'article_count': 0,
            'internal_links': [],
            'categories': []
        }

        try:
            response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            # 1. åŸºæœ¬ä¿¡æ¯
            title = soup.find('title')
            analysis['title'] = title.text.strip() if title else ''

            # 2. Metaä¿¡æ¯
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and 'content' in meta_desc.attrs:
                analysis['meta_description'] = meta_desc['content']

            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
            if meta_keywords and 'content' in meta_keywords.attrs:
                analysis['keywords'] = [k.strip() for k in meta_keywords['content'].split(',')]

            # 3. å†…å®¹ç»“æ„åˆ†æ
            h1_tags = soup.find_all('h1')
            h2_tags = soup.find_all('h2')
            h3_tags = soup.find_all('h3')

            analysis['content_structure'] = {
                'h1_count': len(h1_tags),
                'h2_count': len(h2_tags),
                'h3_count': len(h3_tags),
                'h1_texts': [h.text.strip() for h in h1_tags[:5]],
                'h2_texts': [h.text.strip() for h in h2_tags[:10]]
            }

            # 4. æ£€æµ‹å˜ç°æ–¹å¼
            html_text = response.text.lower()

            if 'adsense' in html_text or 'googlesyndication' in html_text:
                analysis['monetization'].append('Google AdSense')

            if 'amazon-adsystem' in html_text or 'amzn.to' in html_text:
                analysis['monetization'].append('Amazon Associates')

            if 'mediavine' in html_text:
                analysis['monetization'].append('Mediavine')

            if 'ezoic' in html_text:
                analysis['monetization'].append('Ezoic')

            # 5. æŠ€æœ¯æ ˆæ£€æµ‹
            if 'wp-content' in html_text or 'wordpress' in html_text:
                analysis['tech_stack'].append('WordPress')

            if '__next' in html_text or '_next' in html_text:
                analysis['tech_stack'].append('Next.js')

            if 'gatsby' in html_text:
                analysis['tech_stack'].append('Gatsby')

            # 6. æ–‡ç« /å†…å®¹é¡µé¢é“¾æ¥
            article_links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                # è¯†åˆ«æ–‡ç« URLæ¨¡å¼
                if any(pattern in href for pattern in ['/blog/', '/post/', '/article/', '/review/']):
                    article_links.append(href)

            analysis['article_count'] = len(set(article_links))
            analysis['sample_articles'] = list(set(article_links))[:10]

            # 7. åˆ†ç±»/å¯¼èˆª
            nav = soup.find('nav')
            if nav:
                categories = [a.text.strip() for a in nav.find_all('a') if a.text.strip()]
                analysis['categories'] = categories[:15]

            print(f"   âœ… åˆ†æå®Œæˆ")
            print(f"   - æ ‡é¢˜: {analysis['title'][:50]}...")
            print(f"   - å˜ç°æ–¹å¼: {', '.join(analysis['monetization']) if analysis['monetization'] else 'æœªæ£€æµ‹åˆ°'}")
            print(f"   - æŠ€æœ¯æ ˆ: {', '.join(analysis['tech_stack']) if analysis['tech_stack'] else 'æœªæ£€æµ‹åˆ°'}")
            print(f"   - æ–‡ç« æ•°é‡: {analysis['article_count']}")

            return analysis

        except Exception as e:
            print(f"   âŒ åˆ†æå¤±è´¥: {e}")
            return analysis

    def score_keyword(self, keyword):
        """å…³é”®è¯è¯„åˆ†ï¼ˆ0-100ï¼‰"""
        score = 0
        kw_lower = keyword.lower()

        # 1. é•¿åº¦è¯„åˆ†ï¼ˆé•¿å°¾è¯æ›´å¥½ï¼‰
        word_count = len(keyword.split())
        if word_count >= 4:
            score += 25  # é•¿å°¾è¯æœ€ä½³
        elif word_count == 3:
            score += 20
        elif word_count == 2:
            score += 10
        else:
            score += 5

        # 2. å•†ä¸šæ„å›¾è¯„åˆ†
        high_intent = ['buy', 'price', 'cost', 'cheap', 'affordable', 'discount', 'deal']
        medium_intent = ['best', 'top', 'review', 'vs', 'compare', 'alternative']

        for word in high_intent:
            if word in kw_lower:
                score += 30
                break
        else:
            for word in medium_intent:
                if word in kw_lower:
                    score += 20
                    break

        # 3. å†…å®¹ç±»å‹è¯„åˆ†
        question_words = ['how', 'what', 'why', 'when', 'where', 'who', 'which']
        for qw in question_words:
            if kw_lower.startswith(qw):
                score += 15
                break

        # 4. å…·ä½“æ€§è¯„åˆ†
        if any(char.isdigit() for char in keyword):
            score += 10  # åŒ…å«æ•°å­—ï¼ˆå¦‚"top 10"ï¼‰

        # 5. å¹´ä»½è¯„åˆ†ï¼ˆæ—¶æ•ˆæ€§ï¼‰
        current_year = datetime.now().year
        if str(current_year) in keyword or str(current_year-1) in keyword:
            score += 10

        return min(score, 100)

    def generate_site_plan(self, keyword_data, competitor_analysis):
        """æ ¹æ®å…³é”®è¯å’Œç«å“åˆ†æç”Ÿæˆç«™ç‚¹æ–¹æ¡ˆ"""
        print("\n" + "="*60)
        print("ğŸ¯ ç”Ÿæˆç«™ç‚¹å»ºè®¾æ–¹æ¡ˆ")
        print("="*60)

        plan = {
            'recommended_domain': '',
            'niche': '',
            'content_strategy': {},
            'monetization_plan': [],
            'tech_stack': '',
            'initial_articles': []
        }

        # åˆ†ææœ€ä½³åˆ©åŸºå¸‚åœº
        top_keywords = sorted(keyword_data, key=lambda x: x['score'], reverse=True)[:20]

        # æå–å…±åŒä¸»é¢˜
        all_words = []
        for kw in top_keywords:
            all_words.extend(kw['keyword'].lower().split())

        word_freq = Counter(all_words)
        common_words = [w for w, c in word_freq.most_common(10)
                       if w not in ['the', 'a', 'an', 'of', 'to', 'in', 'for', 'and', 'or']]

        plan['niche'] = ' '.join(common_words[:3])

        # åŸŸåå»ºè®®
        domain_base = ''.join(common_words[:2])
        plan['recommended_domain'] = f"{domain_base}hub.com æˆ– {domain_base}guide.com"

        # å†…å®¹ç­–ç•¥
        plan['content_strategy'] = {
            'total_articles': 30,  # ç¬¬ä¸€ä¸ªæœˆç›®æ ‡
            'article_types': {
                'äº§å“è¯„æµ‹': 10,  # "best XXX", "XXX review"
                'å¯¹æ¯”æ–‡ç« ': 5,   # "XXX vs YYY"
                'æŒ‡å—æ•™ç¨‹': 10,  # "how to XXX"
                'åˆ—è¡¨æ–‡ç« ': 5    # "top 10 XXX"
            },
            'publishing_frequency': 'æ¯å¤©1ç¯‡',
            'word_count': '1500-2500å­—/ç¯‡'
        }

        # å˜ç°æ–¹æ¡ˆ
        monetization_methods = set()
        for comp in competitor_analysis:
            monetization_methods.update(comp.get('monetization', []))

        if monetization_methods:
            plan['monetization_plan'] = list(monetization_methods)
        else:
            plan['monetization_plan'] = ['Google AdSense', 'Amazon Associates']

        # æŠ€æœ¯æ ˆæ¨è
        tech_stacks = []
        for comp in competitor_analysis:
            tech_stacks.extend(comp.get('tech_stack', []))

        if 'WordPress' in tech_stacks:
            plan['tech_stack'] = 'WordPress (æœ€å¸¸ç”¨ï¼Œæ’ä»¶ä¸°å¯Œ)'
        elif 'Next.js' in tech_stacks:
            plan['tech_stack'] = 'Next.js (æ€§èƒ½å¥½ï¼ŒSEOå‹å¥½)'
        else:
            plan['tech_stack'] = 'Next.js (æ¨èï¼Œé€‚åˆè‡ªåŠ¨åŒ–)'

        # åˆå§‹æ–‡ç« å»ºè®®
        plan['initial_articles'] = [kw['keyword'] for kw in top_keywords[:10]]

        # æ‰“å°æ–¹æ¡ˆ
        print(f"\nğŸ“Œ åˆ©åŸºå¸‚åœº: {plan['niche']}")
        print(f"ğŸŒ æ¨èåŸŸå: {plan['recommended_domain']}")
        print(f"\nğŸ“ å†…å®¹ç­–ç•¥:")
        print(f"   - æ€»æ–‡ç« æ•°: {plan['content_strategy']['total_articles']}ç¯‡ï¼ˆç¬¬ä¸€ä¸ªæœˆï¼‰")
        print(f"   - å‘å¸ƒé¢‘ç‡: {plan['content_strategy']['publishing_frequency']}")
        print(f"   - æ–‡ç« ç±»å‹:")
        for article_type, count in plan['content_strategy']['article_types'].items():
            print(f"     â€¢ {article_type}: {count}ç¯‡")

        print(f"\nğŸ’° å˜ç°æ–¹å¼: {', '.join(plan['monetization_plan'])}")
        print(f"âš™ï¸  æŠ€æœ¯æ ˆ: {plan['tech_stack']}")

        print(f"\nğŸ“„ å‰10ç¯‡æ–‡ç« æ ‡é¢˜å»ºè®®:")
        for i, title in enumerate(plan['initial_articles'], 1):
            print(f"   {i:2d}. {title}")

        return plan

    def export_to_csv(self, keyword_data, filename='keywords.csv'):
        """å¯¼å‡ºå…³é”®è¯åˆ°CSV"""
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['keyword', 'score', 'word_count'])
            writer.writeheader()
            writer.writerows(keyword_data)

        print(f"\nğŸ’¾ å…³é”®è¯å·²å¯¼å‡ºåˆ°: {filename}")

    def run_complete_workflow(self, seed_keyword, language='en', analyze_competitors=True):
        """å®Œæ•´å·¥ä½œæµ"""
        print("\n" + "="*60)
        print(f"ğŸš€ å¼€å§‹å®Œæ•´å…³é”®è¯æŒ–æ˜æµç¨‹")
        print(f"ğŸ¯ ç§å­å…³é”®è¯: {seed_keyword}")
        print(f"ğŸŒ è¯­è¨€: {language}")
        print("="*60 + "\n")

        # æ­¥éª¤1: æŒ–æ˜å…³é”®è¯
        all_keywords = set()

        if language == 'zh' or language == 'zh-CN':
            # ä¸­æ–‡å¸‚åœº
            google_kws = self.get_google_suggestions(seed_keyword, 'zh-CN')
            baidu_kws = self.get_baidu_suggestions(seed_keyword)
            all_keywords.update(google_kws)
            all_keywords.update(baidu_kws)
        else:
            # è‹±æ–‡å¸‚åœº
            google_kws = self.get_google_suggestions(seed_keyword, language)
            all_keywords.update(google_kws)

        # æ­¥éª¤2: è¯„åˆ†
        print(f"\nâ­ è¯„åˆ† {len(all_keywords)} ä¸ªå…³é”®è¯...")
        keyword_data = []
        for kw in all_keywords:
            score = self.score_keyword(kw)
            keyword_data.append({
                'keyword': kw,
                'score': score,
                'word_count': len(kw.split())
            })

        keyword_data.sort(key=lambda x: x['score'], reverse=True)

        # æ­¥éª¤3: æ˜¾ç¤ºtopå…³é”®è¯
        print(f"\nğŸ† Top 20 å…³é”®è¯:\n")
        for i, kw in enumerate(keyword_data[:20], 1):
            print(f"{i:2d}. [{kw['score']:3d}åˆ†] {kw['keyword']}")

        # æ­¥éª¤4: åˆ†æç«äº‰å¯¹æ‰‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        competitor_analysis = []
        if analyze_competitors:
            print(f"\n{'='*60}")
            print("ğŸ” åˆ†æç«äº‰å¯¹æ‰‹ç½‘ç«™")
            print(f"{'='*60}")

            # è®©ç”¨æˆ·è¾“å…¥ç«äº‰å¯¹æ‰‹URLï¼ˆå› ä¸ºè‡ªåŠ¨æœç´¢Googleå¯èƒ½è¢«å°ï¼‰
            print("\nğŸ’¡ è¯·æ‰‹åŠ¨æœç´¢Googleæ‰¾åˆ°æ’åå‰3çš„ç½‘ç«™ï¼Œç„¶åè¾“å…¥URL")
            print("   (å¦‚æœä¸æƒ³åˆ†æï¼Œç›´æ¥æŒ‰Enterè·³è¿‡)\n")

            competitor_urls = []
            for i in range(3):
                url = input(f"   ç«äº‰å¯¹æ‰‹{i+1} URL: ").strip()
                if url:
                    competitor_urls.append(url)

            for url in competitor_urls:
                analysis = self.analyze_competitor_site(url)
                competitor_analysis.append(analysis)

        # æ­¥éª¤5: ç”Ÿæˆç«™ç‚¹æ–¹æ¡ˆ
        if competitor_analysis:
            plan = self.generate_site_plan(keyword_data, competitor_analysis)

        # æ­¥éª¤6: å¯¼å‡º
        self.export_to_csv(keyword_data, f'{seed_keyword.replace(" ", "_")}_keywords.csv')

        print(f"\n{'='*60}")
        print("âœ… å®Œæ•´æµç¨‹å®Œæˆï¼")
        print(f"{'='*60}\n")

        return {
            'keywords': keyword_data,
            'competitors': competitor_analysis,
            'plan': plan if competitor_analysis else None
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    print("ğŸ¯ å…è´¹å…³é”®è¯æŒ–æ˜ + ç«å“åˆ†æå·¥å…·")
    print("="*60)

    # ä»£ç†è®¾ç½®
    print("\næ˜¯å¦ä½¿ç”¨ä»£ç†è®¿é—®Google? (æ¨è: æ˜¯)")
    use_proxy_input = input("ä½¿ç”¨ä»£ç† (y/n) [é»˜è®¤: y]: ").strip().lower() or 'y'
    use_proxy = use_proxy_input == 'y'

    proxy_port = 7890
    if use_proxy:
        proxy_input = input(f"ä»£ç†ç«¯å£ [é»˜è®¤: {proxy_port}]: ").strip()
        if proxy_input:
            proxy_port = int(proxy_input)

    digger = KeywordDigger(use_proxy=use_proxy, proxy_port=proxy_port)

    # ç”¨æˆ·è¾“å…¥
    seed = input("\nè¯·è¾“å…¥ç§å­å…³é”®è¯ (ä¾‹å¦‚: coffee maker): ").strip()
    lang = input("è¯­è¨€ (en/zh) [é»˜è®¤: en]: ").strip() or 'en'

    # è¿è¡Œå®Œæ•´æµç¨‹
    results = digger.run_complete_workflow(seed, language=lang, analyze_competitors=True)

    print("\nğŸ‰ æ‰€æœ‰æ•°æ®å·²ä¿å­˜ï¼ç°åœ¨ä½ å¯ä»¥:")
    print("   1. æŸ¥çœ‹CSVæ–‡ä»¶è·å–å®Œæ•´å…³é”®è¯åˆ—è¡¨")
    print("   2. æ ¹æ®æ–¹æ¡ˆæ³¨å†ŒåŸŸå")
    print("   3. å¼€å§‹åˆ›å»ºç½‘ç«™å’Œå†…å®¹")
    print("   4. ç”³è¯·å¹¿å‘Šè”ç›Ÿè´¦å·")
